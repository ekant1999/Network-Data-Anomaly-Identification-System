{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97xWDQoV9lJh",
        "outputId": "356c03da-bd30-49f0-d7ed-5f0944492865"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Install dependencies and mount drive\n",
        "# ----------------------------------------------------------------------------\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "greZT4JQ-3SB",
        "outputId": "de1b4706-50a6-4536-a859-0238eaad3ab8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Libraries imported successfully\n"
          ]
        }
      ],
      "source": [
        "# Import libraries\n",
        "# ----------------------------------------------------------------------------\n",
        "import zipfile\n",
        "import re\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from io import BytesIO\n",
        "\n",
        "print(\" Libraries imported successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDf9CtnA-82u",
        "outputId": "d0ac9e2b-113c-4bfe-9c2b-173eb5bfd032"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Output directory: /content/drive/MyDrive/CMPE255-NIDSPROJECT/data\n",
            " Will process 3 files\n"
          ]
        }
      ],
      "source": [
        "#Configuration\n",
        "# ----------------------------------------------------------------------------\n",
        "# MODIFY THIS PATH to match your Drive location\n",
        "ZIP_PATH = \"/content/drive/MyDrive/CMPE255-NIDSPROJECT/GeneratedLabelledFlows.zip\"\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/CMPE255-NIDSPROJECT/data\"\n",
        "\n",
        "TARGET_FILES = [\n",
        "    \"TrafficLabelling/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\",      # DDoS attacks\n",
        "    \"TrafficLabelling/Wednesday-workingHours.pcap_ISCX.csv\",                  # Multiple DoS variants\n",
        "    \"TrafficLabelling/Tuesday-WorkingHours.pcap_ISCX.csv\"                     # Mixed traffic\n",
        "]\n",
        "\n",
        "# Create output directory if it doesn't exist\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "print(f\" Output directory: {OUTPUT_DIR}\")\n",
        "print(f\" Will process {len(TARGET_FILES)} files\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24XrKgDP_DHP",
        "outputId": "b6d8b75a-7397-4246-e67b-1987eae72a68"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Helper functions defined\n"
          ]
        }
      ],
      "source": [
        "# Cell 4: Helper functions\n",
        "# ----------------------------------------------------------------------------\n",
        "def normalize_col(c: str) -> str:\n",
        "    \"\"\"Trim spaces, remove BOM/nbsp, collapse whitespace.\"\"\"\n",
        "    c = (c or \"\")\n",
        "    c = c.replace(\"\\ufeff\", \"\").replace(\"\\xa0\", \" \")\n",
        "    c = re.sub(r\"\\s+\", \" \", c).strip()\n",
        "    return c\n",
        "\n",
        "def find_label_col(cols, name_target=\"label\"):\n",
        "    \"\"\"Find Label column even if spacing/case differs.\"\"\"\n",
        "    for c in cols:\n",
        "        if normalize_col(c).lower() == name_target.lower():\n",
        "            return c\n",
        "    for c in cols:\n",
        "        if name_target.lower() in normalize_col(c).lower():\n",
        "            return c\n",
        "    return None\n",
        "\n",
        "def is_dos_ddos(label: str) -> bool:\n",
        "    \"\"\"Check if label is DoS/DDoS related.\"\"\"\n",
        "    if pd.isna(label):\n",
        "        return False\n",
        "    label_lower = str(label).lower()\n",
        "    dos_keywords = ['dos', 'ddos', 'hulk', 'goldeneye', 'slowloris', 'slowhttptest']\n",
        "    return any(keyword in label_lower for keyword in dos_keywords)\n",
        "\n",
        "print(\" Helper functions defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aKXO8PiK_GT7",
        "outputId": "0fa25dff-0ad2-49a9-ac8f-8158894c885d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Found 8 CSV files in ZIP\n",
            "\n",
            "Files we'll process:\n",
            "  1. Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\n",
            "  2. Wednesday-workingHours.pcap_ISCX.csv\n",
            "  3. Tuesday-WorkingHours.pcap_ISCX.csv\n"
          ]
        }
      ],
      "source": [
        "# Cell 5: Inspect ZIP contents\n",
        "# ----------------------------------------------------------------------------\n",
        "with zipfile.ZipFile(ZIP_PATH, 'r') as zf:\n",
        "    all_csv_files = [f for f in zf.namelist() if f.lower().endswith('.csv')]\n",
        "\n",
        "print(f\" Found {len(all_csv_files)} CSV files in ZIP\\n\")\n",
        "print(\"Files we'll process:\")\n",
        "for i, f in enumerate(TARGET_FILES, 1):\n",
        "    print(f\"  {i}. {f.split('/')[-1]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-K2FSJE_JYB",
        "outputId": "3901a7d1-d583-462e-f794-25768622b80a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "AUTO-DETECTING FILES IN ZIP\n",
            "================================================================================\n",
            "âœ“ Found 8 CSV files in ZIP:\n",
            "\n",
            "  1. TrafficLabelling /Wednesday-workingHours.pcap_ISCX.csv\n",
            "  2. TrafficLabelling /Tuesday-WorkingHours.pcap_ISCX.csv\n",
            "  3. TrafficLabelling /Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv\n",
            "  4. TrafficLabelling /Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv\n",
            "  5. TrafficLabelling /Monday-WorkingHours.pcap_ISCX.csv\n",
            "  6. TrafficLabelling /Friday-WorkingHours-Morning.pcap_ISCX.csv\n",
            "  7. TrafficLabelling /Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\n",
            "  8. TrafficLabelling /Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv\n",
            "\n",
            "================================================================================\n",
            "MATCHING TARGET FILES\n",
            "================================================================================\n",
            "âœ“ Matched 'Friday-WorkingHours-Afternoon-DDos' â†’ TrafficLabelling /Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\n",
            "âœ“ Matched 'Wednesday-workingHours' â†’ TrafficLabelling /Wednesday-workingHours.pcap_ISCX.csv\n",
            "âœ“ Matched 'Tuesday-WorkingHours' â†’ TrafficLabelling /Tuesday-WorkingHours.pcap_ISCX.csv\n",
            "\n",
            "âœ“ Will process 3 files\n",
            "\n",
            "================================================================================\n",
            "PROCESSING MULTIPLE CSV FILES\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            " Processing File 1/3: Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\n",
            "================================================================================\n",
            " Loaded: 225,745 rows Ã— 85 columns\n",
            "âœ“ Normalized columns and added source tracking\n",
            "\n",
            "================================================================================\n",
            " Processing File 2/3: Wednesday-workingHours.pcap_ISCX.csv\n",
            "================================================================================\n",
            " Loaded: 692,703 rows Ã— 85 columns\n",
            "âœ“ Normalized columns and added source tracking\n",
            "\n",
            "================================================================================\n",
            " Processing File 3/3: Tuesday-WorkingHours.pcap_ISCX.csv\n",
            "================================================================================\n",
            " Loaded: 445,909 rows Ã— 85 columns\n",
            "âœ“ Normalized columns and added source tracking\n",
            "\n",
            " Successfully loaded 3 files\n"
          ]
        }
      ],
      "source": [
        "# Cell 6: Select target CSV file\n",
        "# ----------------------------------------------------------------------------\n",
        "# Prioritize files with DoS/DDoS attacks (Wednesday/Friday typically have more attacks)\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"AUTO-DETECTING FILES IN ZIP\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# First, list ALL files in the ZIP to see the actual structure\n",
        "with zipfile.ZipFile(ZIP_PATH, 'r') as zf:\n",
        "    all_files_in_zip = zf.namelist()\n",
        "    csv_files_in_zip = [f for f in all_files_in_zip if f.lower().endswith('.csv')]\n",
        "\n",
        "    print(f\"âœ“ Found {len(csv_files_in_zip)} CSV files in ZIP:\\n\")\n",
        "    for i, f in enumerate(csv_files_in_zip, 1):\n",
        "        print(f\"  {i}. {f}\")\n",
        "\n",
        "# Now match our target files with actual paths in ZIP\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MATCHING TARGET FILES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "target_keywords = [\n",
        "    \"Friday-WorkingHours-Afternoon-DDos\",\n",
        "    \"Wednesday-workingHours\",\n",
        "    \"Tuesday-WorkingHours\"\n",
        "]\n",
        "\n",
        "matched_files = []\n",
        "for keyword in target_keywords:\n",
        "    for csv_file in csv_files_in_zip:\n",
        "        if keyword.lower() in csv_file.lower():\n",
        "            matched_files.append(csv_file)\n",
        "            print(f\"âœ“ Matched '{keyword}' â†’ {csv_file}\")\n",
        "            break\n",
        "    else:\n",
        "        print(f\"  Could not find file matching '{keyword}'\")\n",
        "\n",
        "print(f\"\\nâœ“ Will process {len(matched_files)} files\")\n",
        "\n",
        "# Now process the matched files\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"PROCESSING MULTIPLE CSV FILES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "all_dataframes = []\n",
        "\n",
        "with zipfile.ZipFile(ZIP_PATH, 'r') as zf:\n",
        "    for idx, csv_file in enumerate(matched_files, 1):\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\" Processing File {idx}/{len(matched_files)}: {csv_file.split('/')[-1]}\")\n",
        "        print(f\"{'='*80}\")\n",
        "\n",
        "        # Load CSV\n",
        "        try:\n",
        "            with zf.open(csv_file) as f:\n",
        "                df_temp = pd.read_csv(f, low_memory=False)\n",
        "\n",
        "            print(f\" Loaded: {df_temp.shape[0]:,} rows Ã— {df_temp.shape[1]} columns\")\n",
        "\n",
        "            # Normalize column names\n",
        "            orig_cols = df_temp.columns.tolist()\n",
        "            norm_cols = [normalize_col(c) for c in orig_cols]\n",
        "            rename_map = dict(zip(orig_cols, norm_cols))\n",
        "            df_temp.rename(columns=rename_map, inplace=True)\n",
        "\n",
        "            # Add source file column for tracking\n",
        "            file_name = csv_file.split('/')[-1].replace('.pcap_ISCX.csv', '')\n",
        "            df_temp['source_file'] = file_name\n",
        "\n",
        "            all_dataframes.append(df_temp)\n",
        "            print(f\"âœ“ Normalized columns and added source tracking\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\" Error loading {csv_file}: {e}\")\n",
        "            continue\n",
        "\n",
        "if len(all_dataframes) == 0:\n",
        "    print(\"\\n ERROR: No files were successfully loaded!\")\n",
        "    print(\"Please check the ZIP file structure.\")\n",
        "else:\n",
        "    print(f\"\\n Successfully loaded {len(all_dataframes)} files\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0vX2Iaa_Nwe",
        "outputId": "7fda608d-aa36-41a5-f479-9cc1ef9a15c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "COMBINING ALL DATAFRAMES\n",
            "================================================================================\n",
            " Combined 3 files\n",
            "  Total shape: 1,364,357 rows Ã— 86 columns\n",
            "  Memory usage: 1351.07 MB\n",
            "\n",
            " Rows per source file:\n",
            "source_file\n",
            "Wednesday-workingHours                692703\n",
            "Tuesday-WorkingHours                  445909\n",
            "Friday-WorkingHours-Afternoon-DDos    225745\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Cell 7: Load CSV from ZIP\n",
        "# ----------------------------------------------------------------------------\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"COMBINING ALL DATAFRAMES\")\n",
        "print(f\"{'='*80}\")\n",
        "\n",
        "# Safety check: ensure we have dataframes to combine\n",
        "if len(all_dataframes) == 0:\n",
        "    raise ValueError(\" ERROR: No files were successfully loaded! Cannot proceed.\")\n",
        "\n",
        "df = pd.concat(all_dataframes, ignore_index=True)\n",
        "\n",
        "print(f\" Combined {len(all_dataframes)} files\")\n",
        "print(f\"  Total shape: {df.shape[0]:,} rows Ã— {df.shape[1]} columns\")\n",
        "print(f\"  Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
        "\n",
        "# Show distribution by source file\n",
        "print(\"\\n Rows per source file:\")\n",
        "print(df['source_file'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qO3okbP0_atp"
      },
      "source": [
        "CSV files, especially from different sources, often have messy column names with\n",
        "extra spaces: \" Label \" or \"Flow  Duration\" , Special invisible characters: BOM (Byte Order Mark) \\ufeff, non-breaking spaces \\xa0 , Inconsistent spacing: \"Total Fwd Packets\" vs \"Total  Fwd  Packets\"\n",
        "These issues cause problems when you try to access columns later.\n",
        "1. Get original column names - This creates a list of all column names as they currently exist in the DataFrame.\n",
        "2. Normalize Each Column Name - For example , ' Source IP'â†’'Source IP' , 'Destination Port 'â†’ 'Destination Port'\n",
        "3. Create a Mapping Dictionary - This tells pandas: Replace old name (key) with new name (value).\n",
        "4. Rename the DataFrame Columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FysAgz6A_Qr_",
        "outputId": "177ce675-3e92-43fe-dfbd-fda08f13cbd7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Normalizing column names...\n",
            " Column names normalized\n",
            "\n",
            "================================================================================\n",
            "ALL COLUMN NAMES\n",
            "================================================================================\n",
            "Total columns: 86\n",
            "\n",
            "   1. Flow ID\n",
            "   2. Source IP\n",
            "   3. Source Port\n",
            "   4. Destination IP\n",
            "   5. Destination Port\n",
            "   6. Protocol\n",
            "   7. Timestamp\n",
            "   8. Flow Duration\n",
            "   9. Total Fwd Packets\n",
            "  10. Total Backward Packets\n",
            "  11. Total Length of Fwd Packets\n",
            "  12. Total Length of Bwd Packets\n",
            "  13. Fwd Packet Length Max\n",
            "  14. Fwd Packet Length Min\n",
            "  15. Fwd Packet Length Mean\n",
            "  16. Fwd Packet Length Std\n",
            "  17. Bwd Packet Length Max\n",
            "  18. Bwd Packet Length Min\n",
            "  19. Bwd Packet Length Mean\n",
            "  20. Bwd Packet Length Std\n",
            "  21. Flow Bytes/s\n",
            "  22. Flow Packets/s\n",
            "  23. Flow IAT Mean\n",
            "  24. Flow IAT Std\n",
            "  25. Flow IAT Max\n",
            "  26. Flow IAT Min\n",
            "  27. Fwd IAT Total\n",
            "  28. Fwd IAT Mean\n",
            "  29. Fwd IAT Std\n",
            "  30. Fwd IAT Max\n",
            "  31. Fwd IAT Min\n",
            "  32. Bwd IAT Total\n",
            "  33. Bwd IAT Mean\n",
            "  34. Bwd IAT Std\n",
            "  35. Bwd IAT Max\n",
            "  36. Bwd IAT Min\n",
            "  37. Fwd PSH Flags\n",
            "  38. Bwd PSH Flags\n",
            "  39. Fwd URG Flags\n",
            "  40. Bwd URG Flags\n",
            "  41. Fwd Header Length\n",
            "  42. Bwd Header Length\n",
            "  43. Fwd Packets/s\n",
            "  44. Bwd Packets/s\n",
            "  45. Min Packet Length\n",
            "  46. Max Packet Length\n",
            "  47. Packet Length Mean\n",
            "  48. Packet Length Std\n",
            "  49. Packet Length Variance\n",
            "  50. FIN Flag Count\n",
            "  51. SYN Flag Count\n",
            "  52. RST Flag Count\n",
            "  53. PSH Flag Count\n",
            "  54. ACK Flag Count\n",
            "  55. URG Flag Count\n",
            "  56. CWE Flag Count\n",
            "  57. ECE Flag Count\n",
            "  58. Down/Up Ratio\n",
            "  59. Average Packet Size\n",
            "  60. Avg Fwd Segment Size\n",
            "  61. Avg Bwd Segment Size\n",
            "  62. Fwd Header Length.1\n",
            "  63. Fwd Avg Bytes/Bulk\n",
            "  64. Fwd Avg Packets/Bulk\n",
            "  65. Fwd Avg Bulk Rate\n",
            "  66. Bwd Avg Bytes/Bulk\n",
            "  67. Bwd Avg Packets/Bulk\n",
            "  68. Bwd Avg Bulk Rate\n",
            "  69. Subflow Fwd Packets\n",
            "  70. Subflow Fwd Bytes\n",
            "  71. Subflow Bwd Packets\n",
            "  72. Subflow Bwd Bytes\n",
            "  73. Init_Win_bytes_forward\n",
            "  74. Init_Win_bytes_backward\n",
            "  75. act_data_pkt_fwd\n",
            "  76. min_seg_size_forward\n",
            "  77. Active Mean\n",
            "  78. Active Std\n",
            "  79. Active Max\n",
            "  80. Active Min\n",
            "  81. Idle Mean\n",
            "  82. Idle Std\n",
            "  83. Idle Max\n",
            "  84. Idle Min\n",
            "  85. Label\n",
            "  86. source_file\n"
          ]
        }
      ],
      "source": [
        "# Cell 8: Normalize column names\n",
        "# ----------------------------------------------------------------------------\n",
        "print(\"\\n Normalizing column names...\")\n",
        "\n",
        "orig_cols = df.columns.tolist()\n",
        "norm_cols = [normalize_col(c) for c in orig_cols]\n",
        "rename_map = dict(zip(orig_cols, norm_cols))\n",
        "df.rename(columns=rename_map, inplace=True)\n",
        "\n",
        "print(f\" Column names normalized\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ALL COLUMN NAMES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"Total columns: {len(df.columns)}\\n\")\n",
        "for i, col in enumerate(df.columns, 1):\n",
        "    print(f\"  {i:2d}. {col}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q1YMsIGKL-vT",
        "outputId": "48ecd3e8-44fd-4b32-e877-a174726e3c4f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total columns after normalization: 86\n",
            "\n",
            " Sample of normalized columns:\n",
            "  'Flow ID' - Length: 7, Has spaces at ends: False\n",
            "  'Source IP' - Length: 9, Has spaces at ends: False\n",
            "  'Source Port' - Length: 11, Has spaces at ends: False\n",
            "  'Destination IP' - Length: 14, Has spaces at ends: False\n",
            "  'Destination Port' - Length: 16, Has spaces at ends: False\n",
            "\n",
            " Label column check:\n",
            "  Found: 'Label'\n"
          ]
        }
      ],
      "source": [
        "# Quick verification cell (optional - run between Cell 8 and 9)\n",
        "print(f\"Total columns after normalization: {len(df.columns)}\")\n",
        "print(f\"\\n Sample of normalized columns:\")\n",
        "for col in df.columns[:5]:\n",
        "    print(f\"  '{col}' - Length: {len(col)}, Has spaces at ends: {col != col.strip()}\")\n",
        "\n",
        "print(f\"\\n Label column check:\")\n",
        "label_col = find_label_col(df.columns, \"label\")\n",
        "if label_col:\n",
        "    print(f\"  Found: '{label_col}'\")\n",
        "else:\n",
        "    print(\"   Not found - showing columns with 'label' in name:\")\n",
        "    for col in df.columns:\n",
        "        if 'label' in col.lower():\n",
        "            print(f\"    - '{col}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BfHO_fciAuT2",
        "outputId": "52dac367-390c-4c13-e4f8-144fcfb1e420"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Initial Data Inspection\n",
            "================================================================================\n",
            "Dataset shape: 1,364,357 rows Ã— 86 columns\n",
            "Memory usage: 1351.07 MB\n",
            "\n",
            "Data types distribution:\n",
            "int64      43\n",
            "float64    37\n",
            "object      6\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Cell 9: Initial data inspection\n",
        "# ----------------------------------------------------------------------------\n",
        "print(\"\\n Initial Data Inspection\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Dataset shape: {df.shape[0]:,} rows Ã— {df.shape[1]} columns\")\n",
        "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
        "print(\"\\nData types distribution:\")\n",
        "print(df.dtypes.value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFxMm55oAxGw",
        "outputId": "03c7690c-cc73-4818-ae0a-f6fcd74bd5a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ðŸ” Checking Missing Values Across ALL Columns...\n",
            "================================================================================\n",
            "  Found 1 columns with missing values:\n",
            "\n",
            "      Column  Missing_Count  Missing_Percent\n",
            "Flow Bytes/s           1213         0.088906\n",
            "\n",
            " Missing Data Severity:\n",
            "   Severe (>50%): 0 columns\n",
            "   Moderate (10-50%): 0 columns\n",
            "   Minor (<10%): 1 columns\n"
          ]
        }
      ],
      "source": [
        "# Cell 10: Check for missing values\n",
        "# ----------------------------------------------------------------------------\n",
        "print(\"\\nðŸ” Checking Missing Values Across ALL Columns...\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "missing_counts = df.isnull().sum()\n",
        "missing_percent = (missing_counts / len(df)) * 100\n",
        "missing_df = pd.DataFrame({\n",
        "    'Column': missing_counts.index,\n",
        "    'Missing_Count': missing_counts.values,\n",
        "    'Missing_Percent': missing_percent.values\n",
        "})\n",
        "missing_df = missing_df[missing_df['Missing_Count'] > 0].sort_values('Missing_Percent', ascending=False)\n",
        "\n",
        "if len(missing_df) > 0:\n",
        "    print(f\"  Found {len(missing_df)} columns with missing values:\\n\")\n",
        "    print(missing_df.to_string(index=False))\n",
        "\n",
        "    # Visualize missing data severity\n",
        "    print(\"\\n Missing Data Severity:\")\n",
        "    severe = missing_df[missing_df['Missing_Percent'] > 50]\n",
        "    moderate = missing_df[(missing_df['Missing_Percent'] > 10) & (missing_df['Missing_Percent'] <= 50)]\n",
        "    minor = missing_df[missing_df['Missing_Percent'] <= 10]\n",
        "\n",
        "    print(f\"   Severe (>50%): {len(severe)} columns\")\n",
        "    print(f\"   Moderate (10-50%): {len(moderate)} columns\")\n",
        "    print(f\"   Minor (<10%): {len(minor)} columns\")\n",
        "else:\n",
        "    print(\" No missing values found!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_TtltbLA1KI",
        "outputId": "7a3faf6f-b608-4637-dd2d-217615c78a9a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "  Handling Missing Values...\n",
            "================================================================================\n",
            "\n",
            " Handling 1 columns with moderate/minor missing values:\n",
            "  âœ“ Flow Bytes/s: Filled with median 1039.09 (0.09% missing)\n",
            "\n",
            " Missing value handling complete\n",
            "  Rows: 1,364,357 â†’ 1,364,357 (0 removed)\n",
            "  Columns: 86 â†’ 86 (0 removed)\n"
          ]
        }
      ],
      "source": [
        "# Cell 11: Handle missing values\n",
        "# ----------------------------------------------------------------------------\n",
        "print(\"\\n  Handling Missing Values...\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "initial_rows = len(df)\n",
        "initial_cols = len(df.columns)\n",
        "\n",
        "# Strategy 1: Drop columns with >50% missing values\n",
        "high_missing_cols = missing_df[missing_df['Missing_Percent'] > 50]['Column'].tolist()\n",
        "if high_missing_cols:\n",
        "    print(f\"\\n Dropping {len(high_missing_cols)} columns with >50% missing:\")\n",
        "    for col in high_missing_cols:\n",
        "        pct = missing_df[missing_df['Column'] == col]['Missing_Percent'].values[0]\n",
        "        print(f\"  - {col} ({pct:.1f}% missing)\")\n",
        "    df.drop(columns=high_missing_cols, inplace=True)\n",
        "\n",
        "# Strategy 2: For remaining columns with missing values\n",
        "remaining_missing = df.isnull().sum()\n",
        "remaining_missing = remaining_missing[remaining_missing > 0]\n",
        "\n",
        "if len(remaining_missing) > 0:\n",
        "    print(f\"\\n Handling {len(remaining_missing)} columns with moderate/minor missing values:\")\n",
        "\n",
        "    for col in remaining_missing.index:\n",
        "        missing_count = remaining_missing[col]\n",
        "        missing_pct = (missing_count / len(df)) * 100\n",
        "\n",
        "        if df[col].dtype in ['float64', 'int64']:\n",
        "            # Fill numeric columns with median (using proper assignment to avoid FutureWarning)\n",
        "            median_val = df[col].median()\n",
        "            df[col] = df[col].fillna(median_val)\n",
        "            print(f\"  âœ“ {col}: Filled with median {median_val:.2f} ({missing_pct:.2f}% missing)\")\n",
        "        else:\n",
        "            # For non-numeric, drop rows if <1%, else fill with 'Unknown'\n",
        "            if missing_pct < 1.0:\n",
        "                df.dropna(subset=[col], inplace=True)\n",
        "                print(f\"   {col}: Dropped {missing_count} rows ({missing_pct:.3f}% missing)\")\n",
        "            else:\n",
        "                df[col].fillna('Unknown', inplace=True)\n",
        "                print(f\"   {col}: Filled with 'Unknown' ({missing_pct:.2f}% missing)\")\n",
        "\n",
        "print(f\"\\n Missing value handling complete\")\n",
        "print(f\"  Rows: {initial_rows:,} â†’ {len(df):,} ({initial_rows - len(df):,} removed)\")\n",
        "print(f\"  Columns: {initial_cols} â†’ {len(df.columns)} ({initial_cols - len(df.columns)} removed)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4j5MouUTA5yO",
        "outputId": "f944428a-cff6-445e-8c7d-50c55d83b884"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ðŸ” Checking for Duplicate Rows...\n",
            "================================================================================\n",
            "Total columns in dataset: 86\n",
            "Checking duplicates on: 83 feature columns\n",
            "Excluded from check: Flow ID, Timestamp, source_file\n",
            "\n",
            " Data Type Distribution in Columns Being Checked:\n",
            "  â€¢ int64: 43 columns\n",
            "  â€¢ float64: 37 columns\n",
            "  â€¢ object: 3 columns\n",
            "\n",
            " Attempt 1: Exact matching (including float precision)...\n",
            "  Duplicates found: 9,880 (0.72%)\n",
            "\n",
            " Attempt 2: Rounding floats to 6 decimals (ML-friendly precision)...\n",
            "  Rounding 37 float columns...\n",
            "  Duplicates found after rounding: 9,880 (0.72%)\n",
            "\n",
            "âœ“ Using exact matching (rounding didn't find additional duplicates)\n",
            "\n",
            "================================================================================\n",
            "FINAL RESULT: 9,880 duplicates (0.72%)\n",
            "================================================================================\n",
            "\n",
            " Sample duplicate rows (showing key features + timestamps):\n",
            "          Timestamp                                  Flow ID      Source IP  Destination IP  Source Port  Destination Port  Protocol  Flow Duration  Total Fwd Packets  Total Backward Packets   Label                         source_file\n",
            "405   7/7/2017 3:30  192.168.10.255-192.168.10.17-137-137-17  192.168.10.17  192.168.10.255          137               137        17             22                 13                       0  BENIGN  Friday-WorkingHours-Afternoon-DDos\n",
            "1558  7/7/2017 3:30   192.168.10.17-192.168.10.50-137-137-17  192.168.10.50   192.168.10.17          137               137        17              4                  2                       0  BENIGN  Friday-WorkingHours-Afternoon-DDos\n",
            "3103  7/7/2017 3:31  192.168.10.255-192.168.10.19-137-137-17  192.168.10.19  192.168.10.255          137               137        17             22                 13                       0  BENIGN  Friday-WorkingHours-Afternoon-DDos\n",
            "3382  7/7/2017 3:31   192.168.10.19-192.168.10.50-137-137-17  192.168.10.50   192.168.10.19          137               137        17              3                  2                       0  BENIGN  Friday-WorkingHours-Afternoon-DDos\n",
            "3690  7/7/2017 3:32  192.168.10.255-192.168.10.12-137-137-17  192.168.10.12  192.168.10.255          137               137        17             21                 13                       0  BENIGN  Friday-WorkingHours-Afternoon-DDos\n",
            "3764  7/7/2017 3:32  192.168.10.255-192.168.10.16-137-137-17  192.168.10.16  192.168.10.255          137               137        17             61                 13                       0  BENIGN  Friday-WorkingHours-Afternoon-DDos\n",
            "3850  7/7/2017 3:32  192.168.10.255-192.168.10.19-138-138-17  192.168.10.19  192.168.10.255          138               138        17             25                 13                       0  BENIGN  Friday-WorkingHours-Afternoon-DDos\n",
            "3854  7/7/2017 3:32  192.168.10.255-192.168.10.25-138-138-17  192.168.10.25  192.168.10.255          138               138        17             26                 13                       0  BENIGN  Friday-WorkingHours-Afternoon-DDos\n",
            "3855  7/7/2017 3:32  192.168.10.255-192.168.10.25-137-137-17  192.168.10.25  192.168.10.255          137               137        17             22                 13                       0  BENIGN  Friday-WorkingHours-Afternoon-DDos\n",
            "3934  7/7/2017 3:32   192.168.10.12-192.168.10.50-137-137-17  192.168.10.50   192.168.10.12          137               137        17              3                  2                       0  BENIGN  Friday-WorkingHours-Afternoon-DDos\n",
            "\n",
            " Duplicate Analysis:\n",
            "   Groups with SAME timestamp: 11 (likely data errors)\n",
            "   Groups with DIFFERENT timestamps: 7192 (repeated flow patterns)\n",
            "\n",
            "  Removing duplicates (keeping first occurrence)...\n",
            " Removed 9,880 duplicate rows\n",
            "  Rationale: Feature-identical flows are redundant for ML training\n",
            "  Final shape: 1,354,477 rows Ã— 86 columns\n"
          ]
        }
      ],
      "source": [
        "## Cell 12: Check for duplicate rows with diagnostics\n",
        "# ----------------------------------------------------------------------------\n",
        "print(\"\\nðŸ” Checking for Duplicate Rows...\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "initial_rows = len(df)\n",
        "\n",
        "# Exclude identifier columns that should be unique\n",
        "exclude_cols = ['Flow ID', 'Timestamp', 'source_file']\n",
        "cols_to_check = [c for c in df.columns if c not in exclude_cols]\n",
        "\n",
        "print(f\"Total columns in dataset: {len(df.columns)}\")\n",
        "print(f\"Checking duplicates on: {len(cols_to_check)} feature columns\")\n",
        "print(f\"Excluded from check: {', '.join(exclude_cols)}\")\n",
        "\n",
        "# === DIAGNOSTIC: Check column types ===\n",
        "print(\"\\n Data Type Distribution in Columns Being Checked:\")\n",
        "dtype_counts = df[cols_to_check].dtypes.value_counts()\n",
        "for dtype, count in dtype_counts.items():\n",
        "    print(f\"  â€¢ {dtype}: {count} columns\")\n",
        "\n",
        "# === Try 1: Check with exact matching (including float precision) ===\n",
        "print(\"\\n Attempt 1: Exact matching (including float precision)...\")\n",
        "duplicate_count_exact = df.duplicated(subset=cols_to_check).sum()\n",
        "print(f\"  Duplicates found: {duplicate_count_exact:,} ({duplicate_count_exact/initial_rows*100:.2f}%)\")\n",
        "\n",
        "# === Try 2: Round float columns to reduce precision sensitivity ===\n",
        "print(\"\\n Attempt 2: Rounding floats to 6 decimals (ML-friendly precision)...\")\n",
        "df_temp = df.copy()\n",
        "float_cols = df_temp[cols_to_check].select_dtypes(include=['float64']).columns.tolist()\n",
        "print(f\"  Rounding {len(float_cols)} float columns...\")\n",
        "\n",
        "for col in float_cols:\n",
        "    df_temp[col] = df_temp[col].round(6)\n",
        "\n",
        "duplicate_count_rounded = df_temp.duplicated(subset=cols_to_check).sum()\n",
        "print(f\"  Duplicates found after rounding: {duplicate_count_rounded:,} ({duplicate_count_rounded/initial_rows*100:.2f}%)\")\n",
        "\n",
        "# === Decision: Use the rounded approach if it finds more duplicates ===\n",
        "if duplicate_count_rounded > duplicate_count_exact:\n",
        "    print(\"\\n Using rounded approach (more appropriate for ML)\")\n",
        "    duplicate_count = duplicate_count_rounded\n",
        "    # Apply rounding to original dataframe\n",
        "    for col in float_cols:\n",
        "        df[col] = df[col].round(6)\n",
        "    print(f\"  Applied rounding to {len(float_cols)} float columns in main dataframe\")\n",
        "else:\n",
        "    print(\"\\nâœ“ Using exact matching (rounding didn't find additional duplicates)\")\n",
        "    duplicate_count = duplicate_count_exact\n",
        "\n",
        "# === Process duplicates ===\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(f\"FINAL RESULT: {duplicate_count:,} duplicates ({duplicate_count/initial_rows*100:.2f}%)\")\n",
        "print(f\"{'='*80}\")\n",
        "\n",
        "if duplicate_count > 0:\n",
        "    print(\"\\n Sample duplicate rows (showing key features + timestamps):\")\n",
        "    duplicates_mask = df.duplicated(subset=cols_to_check, keep=False)\n",
        "    duplicates = df[duplicates_mask]\n",
        "\n",
        "    # Show key columns\n",
        "    display_cols = [c for c in ['Timestamp', 'Flow ID', 'Source IP', 'Destination IP',\n",
        "                                  'Source Port', 'Destination Port', 'Protocol',\n",
        "                                  'Flow Duration', 'Total Fwd Packets',\n",
        "                                  'Total Backward Packets', 'Label', 'source_file']\n",
        "                    if c in df.columns]\n",
        "    print(duplicates[display_cols].head(10).to_string())\n",
        "\n",
        "    # Verify: Check if duplicates have different timestamps\n",
        "    if 'Timestamp' in df.columns:\n",
        "        dup_groups = duplicates.groupby(cols_to_check)['Timestamp'].nunique()\n",
        "        same_timestamp_groups = (dup_groups == 1).sum()\n",
        "        diff_timestamp_groups = (dup_groups > 1).sum()\n",
        "\n",
        "        print(f\"\\n Duplicate Analysis:\")\n",
        "        print(f\"   Groups with SAME timestamp: {same_timestamp_groups} (likely data errors)\")\n",
        "        print(f\"   Groups with DIFFERENT timestamps: {diff_timestamp_groups} (repeated flow patterns)\")\n",
        "\n",
        "    # Remove duplicates\n",
        "    print(f\"\\n  Removing duplicates (keeping first occurrence)...\")\n",
        "    df.drop_duplicates(subset=cols_to_check, inplace=True, keep='first')\n",
        "    print(f\" Removed {duplicate_count:,} duplicate rows\")\n",
        "    print(f\"  Rationale: Feature-identical flows are redundant for ML training\")\n",
        "    print(f\"  Final shape: {df.shape[0]:,} rows Ã— {df.shape[1]} columns\")\n",
        "else:\n",
        "    print(\"\\n Analysis: Zero duplicates could mean:\")\n",
        "    print(\"  1. CIC IDS 2017 has genuinely unique flows (high-quality dataset)\")\n",
        "    print(\"  2. Timestamp/Flow ID variations make each row unique\")\n",
        "    print(\"  3. High float precision creates artificial uniqueness\")\n",
        "    print(\"\\n Proceeding with full dataset - no duplicates to remove\")\n",
        "    print(f\"  Final shape: {df.shape[0]:,} rows Ã— {df.shape[1]} columns\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDYKFkqcA80O",
        "outputId": "b96af3b8-8929-4ac1-dd0a-308ff5f908ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "  Label Distribution Analysis\n",
            "================================================================================\n",
            " Found label column: 'Label'\n",
            "\n",
            "All labels in dataset:\n",
            "  BENIGN                                  :    967,555 ( 71.43%) \n",
            "  DoS Hulk                                :    223,488 ( 16.50%)  DoS/DDoS\n",
            "  DDoS                                    :    128,027 (  9.45%)  DoS/DDoS\n",
            "  DoS GoldenEye                           :     10,293 (  0.76%)  DoS/DDoS\n",
            "  FTP-Patator                             :      7,938 (  0.59%) \n",
            "  SSH-Patator                             :      5,897 (  0.44%) \n",
            "  DoS slowloris                           :      5,769 (  0.43%)  DoS/DDoS\n",
            "  DoS Slowhttptest                        :      5,499 (  0.41%)  DoS/DDoS\n",
            "  Heartbleed                              :         11 (  0.00%) \n",
            "\n",
            " Attack Type Summary:\n",
            "   DoS/DDoS: 373,076 (27.54%)\n",
            "   BENIGN: 967,555 (71.43%)\n",
            "   Other attacks: 13,846\n",
            "\n",
            " Label distribution by source file:\n",
            "Label                               BENIGN    DDoS  DoS GoldenEye  DoS Hulk  \\\n",
            "source_file                                                                   \n",
            "Friday-WorkingHours-Afternoon-DDos   97635  128027              0         0   \n",
            "Tuesday-WorkingHours                430887       0              0         0   \n",
            "Wednesday-workingHours              439033       0          10293    223488   \n",
            "\n",
            "Label                               DoS Slowhttptest  DoS slowloris  \\\n",
            "source_file                                                           \n",
            "Friday-WorkingHours-Afternoon-DDos                 0              0   \n",
            "Tuesday-WorkingHours                               0              0   \n",
            "Wednesday-workingHours                          5499           5769   \n",
            "\n",
            "Label                               FTP-Patator  Heartbleed  SSH-Patator  \n",
            "source_file                                                               \n",
            "Friday-WorkingHours-Afternoon-DDos            0           0            0  \n",
            "Tuesday-WorkingHours                       7938           0         5897  \n",
            "Wednesday-workingHours                        0          11            0  \n"
          ]
        }
      ],
      "source": [
        "# Cell 13: Verify label column and display distribution\n",
        "# ----------------------------------------------------------------------------\n",
        "print(\"\\n  Label Distribution Analysis\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "label_col = find_label_col(df.columns, \"label\")\n",
        "\n",
        "if label_col:\n",
        "    print(f\" Found label column: '{label_col}'\\n\")\n",
        "\n",
        "    # Overall label distribution\n",
        "    label_counts = df[label_col].value_counts()\n",
        "    print(\"All labels in dataset:\")\n",
        "    for label, count in label_counts.items():\n",
        "        pct = count / len(df) * 100\n",
        "        is_dos = \" DoS/DDoS\" if is_dos_ddos(label) else \"\"\n",
        "        print(f\"  {label:40s}: {count:10,} ({pct:6.2f}%) {is_dos}\")\n",
        "\n",
        "    # DoS/DDoS specific count\n",
        "    dos_count = df[label_col].apply(is_dos_ddos).sum()\n",
        "    benign_count = df[label_col].str.strip().str.lower().eq('benign').sum()\n",
        "\n",
        "    print(f\"\\n Attack Type Summary:\")\n",
        "    print(f\"   DoS/DDoS: {dos_count:,} ({dos_count/len(df)*100:.2f}%)\")\n",
        "    print(f\"   BENIGN: {benign_count:,} ({benign_count/len(df)*100:.2f}%)\")\n",
        "    print(f\"   Other attacks: {len(df) - dos_count - benign_count:,}\")\n",
        "\n",
        "    # Distribution by source file\n",
        "    print(f\"\\n Label distribution by source file:\")\n",
        "    file_label_dist = pd.crosstab(df['source_file'], df[label_col])\n",
        "    print(file_label_dist)\n",
        "\n",
        "else:\n",
        "    print(\"  No label column found!\")\n",
        "    print(\"Available columns:\", df.columns.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHXvQ2blBDhq",
        "outputId": "940ec1ac-cf02-48fe-8622-a71ae832c39e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Saving Cleaned Data (Missing & Duplicates Handled)...\n",
            "================================================================================\n",
            " Saved to: /content/drive/MyDrive/CMPE255-NIDSPROJECT/data/step1_cleaned_combined_3files.parquet\n",
            "  Shape: 1,354,477 rows Ã— 86 columns\n",
            "  File size: 181.27 MB\n",
            " Saved metadata to: /content/drive/MyDrive/CMPE255-NIDSPROJECT/data/step1_metadata_3files.txt\n"
          ]
        }
      ],
      "source": [
        "# Cell 14: Save cleaned data (Step 1 complete)\n",
        "# ----------------------------------------------------------------------------\n",
        "print(\"\\n Saving Cleaned Data (Missing & Duplicates Handled)...\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Dynamic filename based on number of files processed\n",
        "num_files = df['source_file'].nunique()\n",
        "output_file = os.path.join(OUTPUT_DIR, f\"step1_cleaned_combined_{num_files}files.parquet\")\n",
        "df.to_parquet(output_file, index=False, engine='pyarrow', compression='snappy')\n",
        "\n",
        "print(f\" Saved to: {output_file}\")\n",
        "print(f\"  Shape: {df.shape[0]:,} rows Ã— {df.shape[1]} columns\")\n",
        "\n",
        "file_size_mb = os.path.getsize(output_file) / 1024**2\n",
        "print(f\"  File size: {file_size_mb:.2f} MB\")\n",
        "\n",
        "# Also save a metadata summary\n",
        "metadata = {\n",
        "    'files_processed': matched_files,\n",
        "    'total_rows': len(df),\n",
        "    'total_columns': len(df.columns),\n",
        "    'source_distribution': df['source_file'].value_counts().to_dict()\n",
        "}\n",
        "metadata_file = os.path.join(OUTPUT_DIR, f\"step1_metadata_{num_files}files.txt\")\n",
        "with open(metadata_file, 'w') as f:\n",
        "    f.write(\"=\"*80 + \"\\n\")\n",
        "    f.write(\"PREPROCESSING STEP 1 METADATA\\n\")\n",
        "    f.write(\"=\"*80 + \"\\n\\n\")\n",
        "    f.write(f\"Files Processed:\\n\")\n",
        "    for i, file in enumerate(metadata['files_processed'], 1):\n",
        "        f.write(f\"  {i}. {file}\\n\")\n",
        "    f.write(f\"\\nTotal Rows: {metadata['total_rows']:,}\\n\")\n",
        "    f.write(f\"Total Columns: {metadata['total_columns']}\\n\")\n",
        "    f.write(f\"\\nRows per Source File:\\n\")\n",
        "    for source, count in metadata['source_distribution'].items():\n",
        "        f.write(f\"  {source}: {count:,}\\n\")\n",
        "\n",
        "print(f\" Saved metadata to: {metadata_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkncZ7-9Uk1Z"
      },
      "source": [
        " Saved interim output as Parquet for:\n",
        "1. Fast reload if Colab disconnects (avoid re-running 10+ min cleaning)\n",
        "2. Parquet = 60% smaller + 10x faster than CSV + preserves data types\n",
        "3. Reproducibility - versioned checkpoints for each preprocessing step\n",
        "4. Experimentation - try different approaches on same clean baseline\n",
        "5. Best practice - industry standard for ML pipelines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s2SvU3LwBHzF",
        "outputId": "0fdfdd80-5cbd-433a-a3ae-00bbe3320d3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Final Dataset Summary (Step 1 Complete)\n",
            "================================================================================\n",
            "Total rows: 1,354,477\n",
            "Total columns: 86\n",
            "Files combined: 3\n",
            "Files: Friday-WorkingHours-Afternoon-DDos, Wednesday-workingHours, Tuesday-WorkingHours\n",
            "\n",
            "Numeric columns summary:\n",
            "        Source Port  Destination Port      Protocol  Flow Duration  \\\n",
            "count  1.354477e+06      1.354477e+06  1.354477e+06   1.354477e+06   \n",
            "mean   4.119804e+04      7.308700e+03  9.517292e+00   2.057618e+07   \n",
            "std    2.172930e+04      1.786470e+04  5.135087e+00   3.805836e+07   \n",
            "min    0.000000e+00      0.000000e+00  0.000000e+00  -4.000000e+00   \n",
            "25%    3.291600e+04      5.300000e+01  6.000000e+00   2.280000e+02   \n",
            "50%    5.064500e+04      8.000000e+01  6.000000e+00   6.943300e+04   \n",
            "75%    5.791000e+04      4.430000e+02  1.700000e+01   9.332258e+06   \n",
            "max    6.553500e+04      6.553200e+04  1.700000e+01   1.200000e+08   \n",
            "\n",
            "       Total Fwd Packets  Total Backward Packets  Total Length of Fwd Packets  \\\n",
            "count       1.354477e+06            1.354477e+06                 1.354477e+06   \n",
            "mean        9.561748e+00            1.056395e+01                 6.138882e+02   \n",
            "std         7.301208e+02            9.735500e+02                 5.640586e+03   \n",
            "min         1.000000e+00            0.000000e+00                 0.000000e+00   \n",
            "25%         2.000000e+00            1.000000e+00                 2.400000e+01   \n",
            "50%         2.000000e+00            2.000000e+00                 6.600000e+01   \n",
            "75%         6.000000e+00            6.000000e+00                 3.440000e+02   \n",
            "max         2.064460e+05            2.760720e+05                 2.428415e+06   \n",
            "\n",
            "       Total Length of Bwd Packets  Fwd Packet Length Max  \\\n",
            "count                 1.354477e+06           1.354477e+06   \n",
            "mean                  1.688330e+04           2.661608e+02   \n",
            "std                   2.199740e+06           9.243043e+02   \n",
            "min                   0.000000e+00           0.000000e+00   \n",
            "25%                   0.000000e+00           6.000000e+00   \n",
            "50%                   1.660000e+02           4.000000e+01   \n",
            "75%                   4.425000e+03           3.020000e+02   \n",
            "max                   6.270000e+08           2.482000e+04   \n",
            "\n",
            "       Fwd Packet Length Min  ...  act_data_pkt_fwd  min_seg_size_forward  \\\n",
            "count           1.354477e+06  ...      1.354477e+06          1.354477e+06   \n",
            "mean            1.897901e+01  ...      4.360155e+00         -4.334424e+03   \n",
            "std             7.929359e+01  ...      5.114725e+02          1.529955e+06   \n",
            "min             0.000000e+00  ...      0.000000e+00         -5.368707e+08   \n",
            "25%             0.000000e+00  ...      0.000000e+00          2.000000e+01   \n",
            "50%             0.000000e+00  ...      1.000000e+00          2.000000e+01   \n",
            "75%             3.400000e+01  ...      3.000000e+00          3.200000e+01   \n",
            "max             2.065000e+03  ...      1.971240e+05          1.380000e+02   \n",
            "\n",
            "        Active Mean    Active Std    Active Max    Active Min     Idle Mean  \\\n",
            "count  1.354477e+06  1.354477e+06  1.354477e+06  1.354477e+06  1.354477e+06   \n",
            "mean   1.019001e+05  4.187941e+04  1.719663e+05  7.779587e+04  1.411298e+07   \n",
            "std    6.966995e+05  4.064798e+05  1.053937e+06  6.299610e+05  3.077767e+07   \n",
            "min    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
            "25%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
            "50%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
            "75%    8.800000e+01  0.000000e+00  8.900000e+01  5.800000e+01  6.648121e+06   \n",
            "max    1.070000e+08  7.420000e+07  1.070000e+08  1.070000e+08  1.200000e+08   \n",
            "\n",
            "           Idle Std      Idle Max      Idle Min  \n",
            "count  1.354477e+06  1.354477e+06  1.354477e+06  \n",
            "mean   8.938770e+05  1.478598e+07  1.344306e+07  \n",
            "std    6.341742e+06  3.170914e+07  3.052646e+07  \n",
            "min    0.000000e+00  0.000000e+00  0.000000e+00  \n",
            "25%    0.000000e+00  0.000000e+00  0.000000e+00  \n",
            "50%    0.000000e+00  0.000000e+00  0.000000e+00  \n",
            "75%    0.000000e+00  6.669672e+06  5.766149e+06  \n",
            "max    7.690000e+07  1.200000e+08  1.200000e+08  \n",
            "\n",
            "[8 rows x 80 columns]\n",
            "\n",
            " Step 1 Complete: Missing values and duplicates handled!\n",
            " Combined 3 CSV files focused on DoS/DDoS attacks\n",
            "Next step: Outlier detection and feature engineering\n"
          ]
        }
      ],
      "source": [
        "# Cell 15: Summary statistics\n",
        "# ----------------------------------------------------------------------------\n",
        "print(\"\\n Final Dataset Summary (Step 1 Complete)\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Total rows: {len(df):,}\")\n",
        "print(f\"Total columns: {len(df.columns)}\")\n",
        "print(f\"Files combined: {df['source_file'].nunique()}\")\n",
        "print(f\"Files: {', '.join(df['source_file'].unique())}\")\n",
        "print(f\"\\nNumeric columns summary:\")\n",
        "print(df.describe())\n",
        "\n",
        "print(\"\\n Step 1 Complete: Missing values and duplicates handled!\")\n",
        "print(f\" Combined {len(matched_files)} CSV files focused on DoS/DDoS attacks\")\n",
        "print(\"Next step: Outlier detection and feature engineering\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JNI8x-B_Mexw",
        "outputId": "9386a314-aa90-451f-a59b-64b4d576b67a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Checking directory: /content/drive/MyDrive/CMPE255-NIDSPROJECT/data\n",
            "\n",
            "================================================================================\n",
            "STEP 2: OUTLIER DETECTION & REMOVAL\n",
            "================================================================================\n",
            "\n",
            " Loading Step 1 cleaned data...\n",
            " Loaded: 1,354,477 rows Ã— 86 columns\n",
            "Memory usage: 1341.30 MB\n"
          ]
        }
      ],
      "source": [
        "# Outlier detection and Removal - Import libraries and load Step 1 output\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "data_dir = '/content/drive/MyDrive/CMPE255-NIDSPROJECT/data'\n",
        "print(f\"Checking directory: {data_dir}\")\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 2: OUTLIER DETECTION & REMOVAL\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\n Loading Step 1 cleaned data...\")\n",
        "df = pd.read_parquet('/content/drive/MyDrive/CMPE255-NIDSPROJECT/data/step1_cleaned_combined_3files.parquet')\n",
        "\n",
        "print(f\" Loaded: {df.shape[0]:,} rows Ã— {df.shape[1]} columns\")\n",
        "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
        "\n",
        "initial_rows = len(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6eQfQ98RNce",
        "outputId": "035d5c05-301e-46fc-db5b-38da9634af3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "IDENTIFYING NUMERIC COLUMNS\n",
            "================================================================================\n",
            "Total numeric columns for outlier detection: 80\n",
            "\n",
            "First 15 numeric columns:\n",
            "   1. Source Port\n",
            "   2. Destination Port\n",
            "   3. Protocol\n",
            "   4. Flow Duration\n",
            "   5. Total Fwd Packets\n",
            "   6. Total Backward Packets\n",
            "   7. Total Length of Fwd Packets\n",
            "   8. Total Length of Bwd Packets\n",
            "   9. Fwd Packet Length Max\n",
            "  10. Fwd Packet Length Min\n",
            "  11. Fwd Packet Length Mean\n",
            "  12. Fwd Packet Length Std\n",
            "  13. Bwd Packet Length Max\n",
            "  14. Bwd Packet Length Min\n",
            "  15. Bwd Packet Length Mean\n"
          ]
        }
      ],
      "source": [
        "#Identify numeric columns for outlier detection\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"IDENTIFYING NUMERIC COLUMNS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Get numeric columns (exclude ID, timestamp, categorical)\n",
        "exclude_cols = ['Flow ID', 'Timestamp', 'Source IP', 'Destination IP', 'Label', 'source_file']\n",
        "numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "numeric_cols = [col for col in numeric_cols if col not in exclude_cols]\n",
        "\n",
        "print(f\"Total numeric columns for outlier detection: {len(numeric_cols)}\")\n",
        "print(f\"\\nFirst 15 numeric columns:\")\n",
        "for i, col in enumerate(numeric_cols[:15], 1):\n",
        "    print(f\"  {i:2d}. {col}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UobpajeVRVVo",
        "outputId": "7e896ba9-7a9e-4b32-cf92-a7ac0422b794"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "STEP 2: OUTLIER DETECTION & REMOVAL\n",
            "================================================================================\n",
            "\n",
            " Loading Step 1 cleaned data...\n",
            " Loaded: 1,354,477 rows Ã— 86 columns\n",
            "Memory usage: 1341.30 MB\n",
            "\n",
            "================================================================================\n",
            "STEP 2.1: DETECTING DOMAIN VIOLATIONS (Negative Values)\n",
            "================================================================================\n",
            "Checking 23 columns that must be non-negative:\n",
            "\n",
            "  Flow Duration: 40 negative values (0.003%)\n",
            "  Flow Bytes/s: 18 negative values (0.001%)\n",
            "  Flow Packets/s: 40 negative values (0.003%)\n",
            "  min_seg_size_forward: 12 negative values (0.001%)\n",
            "\n",
            " Fixing 4 columns with negative values...\n",
            "   Flow Duration: Removed 40 rows\n",
            "   Flow Bytes/s: Removed 0 rows\n",
            "   Flow Packets/s: Removed 0 rows\n",
            "   min_seg_size_forward: Removed 12 rows\n",
            "\n",
            " Domain violations fixed\n",
            "  Rows: 1,354,477 â†’ 1,354,425 (52 removed)\n"
          ]
        }
      ],
      "source": [
        "#Detect and fix negative values <<TBD>>\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 2: OUTLIER DETECTION & REMOVAL\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\n Loading Step 1 cleaned data...\")\n",
        "df = pd.read_parquet('/content/drive/MyDrive/CMPE255-NIDSPROJECT/data/step1_cleaned_combined_3files.parquet')\n",
        "\n",
        "print(f\" Loaded: {df.shape[0]:,} rows Ã— {df.shape[1]} columns\")\n",
        "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
        "\n",
        "initial_rows = len(df)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 2.1: DETECTING DOMAIN VIOLATIONS (Negative Values)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Features that CANNOT be negative (domain knowledge)\n",
        "must_be_positive = [\n",
        "    'Flow Duration', 'Total Fwd Packets', 'Total Backward Packets',\n",
        "    'Total Length of Fwd Packets', 'Total Length of Bwd Packets',\n",
        "    'Fwd Packet Length Max', 'Fwd Packet Length Min', 'Fwd Packet Length Mean',\n",
        "    'Bwd Packet Length Max', 'Bwd Packet Length Min', 'Bwd Packet Length Mean',\n",
        "    'Flow Bytes/s', 'Flow Packets/s', 'Fwd Packets/s', 'Bwd Packets/s',\n",
        "    'Packet Length Min', 'Packet Length Max', 'Packet Length Mean',\n",
        "    'min_seg_size_forward', 'Active Mean', 'Active Max', 'Active Min',\n",
        "    'Idle Mean', 'Idle Max', 'Idle Min'\n",
        "]\n",
        "\n",
        "# Find columns that exist in dataframe\n",
        "must_be_positive = [col for col in must_be_positive if col in df.columns]\n",
        "\n",
        "print(f\"Checking {len(must_be_positive)} columns that must be non-negative:\\n\")\n",
        "\n",
        "negative_issues = {}\n",
        "for col in must_be_positive:\n",
        "    negative_count = (df[col] < 0).sum()\n",
        "    if negative_count > 0:\n",
        "        negative_issues[col] = negative_count\n",
        "        negative_pct = (negative_count / len(df)) * 100\n",
        "        print(f\"  {col}: {negative_count:,} negative values ({negative_pct:.3f}%)\")\n",
        "\n",
        "if negative_issues:\n",
        "    print(f\"\\n Fixing {len(negative_issues)} columns with negative values...\")\n",
        "\n",
        "    for col in negative_issues.keys():\n",
        "        # Strategy: Remove rows with negative values (data quality issues)\n",
        "        rows_before = len(df)\n",
        "        df = df[df[col] >= 0]\n",
        "        rows_removed = rows_before - len(df)\n",
        "        print(f\"   {col}: Removed {rows_removed:,} rows\")\n",
        "\n",
        "    print(f\"\\n Domain violations fixed\")\n",
        "    print(f\"  Rows: {initial_rows:,} â†’ {len(df):,} ({initial_rows - len(df):,} removed)\")\n",
        "else:\n",
        "    print(\"\\n No negative values found in columns that must be positive!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjFg-B8ZRXwj",
        "outputId": "c738299e-41eb-4e42-ef45-1177ee4aab19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "STEP 2.2: HANDLING ZERO-DURATION FLOWS\n",
            "================================================================================\n",
            "Zero-duration flows: 1,570 (0.12%)\n",
            "\n",
            " Analyzing zero-duration flows:\n",
            "   With packets: 1,570\n",
            "   Without packets: 0\n",
            "\n",
            "  Removing zero-duration flows (causes division by zero issues)...\n",
            "  Removed 1,570 zero-duration flows\n",
            "  Rows: 1,354,425 â†’ 1,352,855\n"
          ]
        }
      ],
      "source": [
        "#Detect zero-duration flows <<TBD>>\n",
        "#A zero-duration flow is a network connection\n",
        "#where the time difference between the first and last packet is 0 microseconds (or effectively 0).\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 2.2: HANDLING ZERO-DURATION FLOWS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "if 'Flow Duration' in df.columns:\n",
        "    zero_duration = (df['Flow Duration'] == 0).sum()\n",
        "    zero_duration_pct = (zero_duration / len(df)) * 100\n",
        "\n",
        "    print(f\"Zero-duration flows: {zero_duration:,} ({zero_duration_pct:.2f}%)\")\n",
        "\n",
        "    if zero_duration > 0:\n",
        "        print(\"\\n Analyzing zero-duration flows:\")\n",
        "        zero_flows = df[df['Flow Duration'] == 0]\n",
        "\n",
        "        # Check if they have packets\n",
        "        if 'Total Fwd Packets' in df.columns and 'Total Backward Packets' in df.columns:\n",
        "            zero_with_packets = ((zero_flows['Total Fwd Packets'] > 0) |\n",
        "                                 (zero_flows['Total Backward Packets'] > 0)).sum()\n",
        "            print(f\"   With packets: {zero_with_packets:,}\")\n",
        "            print(f\"   Without packets: {zero_duration - zero_with_packets:,}\")\n",
        "\n",
        "        # Decision: Remove zero-duration flows (problematic for rate calculations)\n",
        "        print(\"\\n  Removing zero-duration flows (causes division by zero issues)...\")\n",
        "        rows_before = len(df)\n",
        "        df = df[df['Flow Duration'] > 0]\n",
        "        rows_removed = rows_before - len(df)\n",
        "        print(f\"  Removed {rows_removed:,} zero-duration flows\")\n",
        "        print(f\"  Rows: {rows_before:,} â†’ {len(df):,}\")\n",
        "else:\n",
        "    print(\"  Flow Duration column not found\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7TkcXmqrRr1O",
        "outputId": "ad28684b-701a-4193-b7c1-0a0647ec5c5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "STEP 2.3: STATISTICAL OUTLIER DETECTION (IQR Method)\n",
            "================================================================================\n",
            "Using IQR (Interquartile Range) method to detect extreme outliers\n",
            "Outliers defined as: values < Q1 - 3*IQR or values > Q3 + 3*IQR\n",
            "(Using 3*IQR instead of 1.5*IQR to be less aggressive)\n",
            "\n",
            " Columns with outliers (top 20):\n",
            "\n",
            "                Column  Outliers Percentage\n",
            "            Active Min    335417     24.79%\n",
            "            Active Max    332297     24.56%\n",
            "           Active Mean    332254     24.56%\n",
            "        PSH Flag Count    321621     23.77%\n",
            "         Fwd IAT Total    289375     21.39%\n",
            "          Flow Bytes/s    284870     21.06%\n",
            "         Flow Duration    283972     20.99%\n",
            "         Bwd Packets/s    281952     20.84%\n",
            "           Bwd IAT Max    281456     20.80%\n",
            "          Bwd IAT Mean    274297     20.28%\n",
            "         Bwd IAT Total    272398     20.14%\n",
            "Packet Length Variance    264638     19.56%\n",
            "           Bwd IAT Std    262271     19.39%\n",
            "           Fwd IAT Min    245841     18.17%\n",
            "          Fwd IAT Mean    224686     16.61%\n",
            "          Flow IAT Min    224053     16.56%\n",
            "           Fwd IAT Max    223919     16.55%\n",
            "          Flow IAT Max    221571     16.38%\n",
            "              Idle Max    220162     16.27%\n",
            "             Idle Mean    214470     15.85%\n"
          ]
        }
      ],
      "source": [
        "# outlier detection using IQR method <<TBD>>\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 2.3: STATISTICAL OUTLIER DETECTION (IQR Method)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"Using IQR (Interquartile Range) method to detect extreme outliers\")\n",
        "print(\"Outliers defined as: values < Q1 - 3*IQR or values > Q3 + 3*IQR\")\n",
        "print(\"(Using 3*IQR instead of 1.5*IQR to be less aggressive)\\n\")\n",
        "\n",
        "outlier_summary = []\n",
        "\n",
        "for col in numeric_cols:\n",
        "    if col not in df.columns:\n",
        "        continue\n",
        "\n",
        "    # Calculate IQR\n",
        "    Q1 = df[col].quantile(0.25)\n",
        "    Q3 = df[col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "\n",
        "    # Define outlier bounds (3*IQR for less aggressive removal)\n",
        "    lower_bound = Q1 - 3 * IQR\n",
        "    upper_bound = Q3 + 3 * IQR\n",
        "\n",
        "    # Count outliers\n",
        "    outliers_lower = (df[col] < lower_bound).sum()\n",
        "    outliers_upper = (df[col] > upper_bound).sum()\n",
        "    total_outliers = outliers_lower + outliers_upper\n",
        "\n",
        "    if total_outliers > 0:\n",
        "        outlier_pct = (total_outliers / len(df)) * 100\n",
        "        outlier_summary.append({\n",
        "            'Column': col,\n",
        "            'Outliers': total_outliers,\n",
        "            'Percentage': outlier_pct,\n",
        "            'Lower_Bound': lower_bound,\n",
        "            'Upper_Bound': upper_bound,\n",
        "            'Q1': Q1,\n",
        "            'Q3': Q3\n",
        "        })\n",
        "\n",
        "# Convert to DataFrame and sort by percentage\n",
        "outlier_df = pd.DataFrame(outlier_summary).sort_values('Percentage', ascending=False)\n",
        "\n",
        "print(f\" Columns with outliers (top 20):\\n\")\n",
        "if len(outlier_df) > 0:\n",
        "    display_df = outlier_df.head(20).copy()\n",
        "    display_df['Percentage'] = display_df['Percentage'].apply(lambda x: f\"{x:.2f}%\")\n",
        "    print(display_df[['Column', 'Outliers', 'Percentage']].to_string(index=False))\n",
        "else:\n",
        "    print(\" No statistical outliers detected!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ba_Z8FxtR5-K",
        "outputId": "4df76efd-d3ef-4981-fc3e-e0184273a4ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "STEP 2.4: REMOVING EXTREME OUTLIERS\n",
            "================================================================================\n",
            "Removing outliers from 7 critical columns:\n",
            "   Flow Duration\n",
            "   Total Fwd Packets\n",
            "   Total Backward Packets\n",
            "   Total Length of Fwd Packets\n",
            "   Total Length of Bwd Packets\n",
            "   Flow Bytes/s\n",
            "   Flow Packets/s\n",
            "  Flow Duration: 283,972 outliers\n",
            "  Total Fwd Packets: 66,507 outliers\n",
            "  Total Backward Packets: 47,248 outliers\n",
            "  Total Length of Fwd Packets: 99,218 outliers\n",
            "  Total Length of Bwd Packets: 33,437 outliers\n",
            "  Flow Bytes/s: 284,870 outliers\n",
            "  Flow Packets/s: 116,355 outliers\n",
            "\n",
            " Outlier removal complete\n",
            "  Removed: 682,156 rows (50.42%)\n",
            "  Remaining: 670,699 rows\n"
          ]
        }
      ],
      "source": [
        "# Remove extreme outliers <<TBD>>\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 2.4: REMOVING EXTREME OUTLIERS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "rows_before_outlier_removal = len(df)\n",
        "\n",
        "# Strategy: Mark rows with outliers in critical columns only\n",
        "# Not all columns - focus on flow characteristics that matter most for DoS/DDoS\n",
        "\n",
        "critical_cols = [\n",
        "    'Flow Duration', 'Total Fwd Packets', 'Total Backward Packets',\n",
        "    'Total Length of Fwd Packets', 'Total Length of Bwd Packets',\n",
        "    'Flow Bytes/s', 'Flow Packets/s'\n",
        "]\n",
        "critical_cols = [col for col in critical_cols if col in df.columns]\n",
        "\n",
        "print(f\"Removing outliers from {len(critical_cols)} critical columns:\")\n",
        "for col in critical_cols:\n",
        "    print(f\"   {col}\")\n",
        "\n",
        "outlier_mask = pd.Series([False] * len(df), index=df.index)\n",
        "\n",
        "for col in critical_cols:\n",
        "    Q1 = df[col].quantile(0.25)\n",
        "    Q3 = df[col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "\n",
        "    lower_bound = Q1 - 3 * IQR\n",
        "    upper_bound = Q3 + 3 * IQR\n",
        "\n",
        "    # Mark rows with outliers\n",
        "    col_outliers = (df[col] < lower_bound) | (df[col] > upper_bound)\n",
        "    outlier_mask = outlier_mask | col_outliers\n",
        "\n",
        "    if col_outliers.sum() > 0:\n",
        "        print(f\"  {col}: {col_outliers.sum():,} outliers\")\n",
        "\n",
        "# Remove rows marked as outliers\n",
        "df = df[~outlier_mask]\n",
        "rows_removed = rows_before_outlier_removal - len(df)\n",
        "removal_pct = (rows_removed / rows_before_outlier_removal) * 100\n",
        "\n",
        "print(f\"\\n Outlier removal complete\")\n",
        "print(f\"  Removed: {rows_removed:,} rows ({removal_pct:.2f}%)\")\n",
        "print(f\"  Remaining: {len(df):,} rows\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AYVrWj6RR6vY",
        "outputId": "b7d93ec0-50ab-4877-c0d7-60ce5fae65a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "STEP 2.5: DOMAIN CONSTRAINT VALIDATION\n",
            "================================================================================\n",
            "Applying domain-specific validation rules:\n",
            "\n",
            " All domain constraints satisfied!\n",
            "  Current shape: 670,699 rows Ã— 86 columns\n"
          ]
        }
      ],
      "source": [
        "#Validate data ranges (domain constraints)\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 2.5: DOMAIN CONSTRAINT VALIDATION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"Applying domain-specific validation rules:\\n\")\n",
        "\n",
        "validation_issues = 0\n",
        "\n",
        "# Rule 1: Ports should be 0-65535\n",
        "for port_col in ['Source Port', 'Destination Port']:\n",
        "    if port_col in df.columns:\n",
        "        invalid_ports = ((df[port_col] < 0) | (df[port_col] > 65535)).sum()\n",
        "        if invalid_ports > 0:\n",
        "            print(f\"  {port_col}: {invalid_ports:,} invalid values\")\n",
        "            df = df[(df[port_col] >= 0) & (df[port_col] <= 65535)]\n",
        "            validation_issues += invalid_ports\n",
        "\n",
        "# Rule 2: Protocol should be valid (0-255 typically)\n",
        "if 'Protocol' in df.columns:\n",
        "    invalid_protocol = ((df['Protocol'] < 0) | (df['Protocol'] > 255)).sum()\n",
        "    if invalid_protocol > 0:\n",
        "        print(f\"  Protocol: {invalid_protocol:,} invalid values\")\n",
        "        df = df[(df['Protocol'] >= 0) & (df['Protocol'] <= 255)]\n",
        "        validation_issues += invalid_protocol\n",
        "\n",
        "# Rule 3: Packet counts should make sense\n",
        "if 'Total Fwd Packets' in df.columns and 'Total Backward Packets' in df.columns:\n",
        "    both_zero = ((df['Total Fwd Packets'] == 0) & (df['Total Backward Packets'] == 0)).sum()\n",
        "    if both_zero > 0:\n",
        "        print(f\"  Flows with zero packets (both directions): {both_zero:,}\")\n",
        "        df = df[~((df['Total Fwd Packets'] == 0) & (df['Total Backward Packets'] == 0))]\n",
        "        validation_issues += both_zero\n",
        "\n",
        "if validation_issues > 0:\n",
        "    print(f\"\\n Validation complete: Removed {validation_issues:,} rows with constraint violations\")\n",
        "else:\n",
        "    print(\" All domain constraints satisfied!\")\n",
        "\n",
        "print(f\"  Current shape: {len(df):,} rows Ã— {len(df.columns)} columns\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aCoFJf1iR-C1",
        "outputId": "cf1a3d57-ad09-4975-ded4-47cd82ed339c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "STEP 2 STATISTICS: BEFORE vs AFTER\n",
            "================================================================================\n",
            "\n",
            "Rows removed summary:\n",
            "  Initial (Step 1 output):     1,354,477\n",
            "  After negative value removal: 670,699\n",
            "  Total removed in Step 2:      683,778 (50.48%)\n",
            "  Final (Step 2 output):        670,699\n",
            "\n",
            " Key columns - statistics after cleaning:\n",
            "       Flow Duration  Total Fwd Packets  Total Backward Packets  \\\n",
            "count   6.706990e+05      670699.000000           670699.000000   \n",
            "mean    1.965673e+06           2.988645                2.277754   \n",
            "std     4.443902e+06           2.455554                2.527590   \n",
            "min     3.300000e+01           1.000000                0.000000   \n",
            "25%     2.404600e+04           1.000000                1.000000   \n",
            "50%     6.885800e+04           2.000000                1.000000   \n",
            "75%     1.357050e+06           4.000000                3.000000   \n",
            "max     3.749164e+07          18.000000               21.000000   \n",
            "\n",
            "        Flow Bytes/s  Flow Packets/s  \n",
            "count  670699.000000   670699.000000  \n",
            "mean     6497.095200     3591.424811  \n",
            "std     13433.949805    10158.365155  \n",
            "min         0.000000        0.053754  \n",
            "25%         3.145610        4.937587  \n",
            "50%      1508.605207       48.394707  \n",
            "75%      6650.633843      143.638175  \n",
            "max     87330.900350    60975.609760  \n"
          ]
        }
      ],
      "source": [
        "#Summary statistics after outlier removal <<TBD>>\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 2 STATISTICS: BEFORE vs AFTER\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\nRows removed summary:\")\n",
        "print(f\"  Initial (Step 1 output):     {initial_rows:,}\")\n",
        "print(f\"  After negative value removal: {len(df):,}\")\n",
        "print(f\"  Total removed in Step 2:      {initial_rows - len(df):,} ({(initial_rows - len(df))/initial_rows*100:.2f}%)\")\n",
        "print(f\"  Final (Step 2 output):        {len(df):,}\")\n",
        "\n",
        "# Show improved statistics for key columns\n",
        "print(f\"\\n Key columns - statistics after cleaning:\")\n",
        "key_cols = ['Flow Duration', 'Total Fwd Packets', 'Total Backward Packets',\n",
        "            'Flow Bytes/s', 'Flow Packets/s']\n",
        "key_cols = [col for col in key_cols if col in df.columns]\n",
        "\n",
        "if key_cols:\n",
        "    print(df[key_cols].describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p18FPiZ5SDja",
        "outputId": "57aa0bb2-d129-4b12-a206-e5b3c93198fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "LABEL DISTRIBUTION AFTER OUTLIER REMOVAL\n",
            "================================================================================\n",
            "\n",
            "Label counts:\n",
            "  BENIGN                        :    522,004 ( 77.83%)\n",
            "  DDoS                          :     89,988 ( 13.42%)\n",
            "  DoS Hulk                      :     38,241 (  5.70%)\n",
            "  DoS GoldenEye                 :      8,196 (  1.22%)\n",
            "  FTP-Patator                   :      6,434 (  0.96%)\n",
            "  SSH-Patator                   :      2,876 (  0.43%)\n",
            "  DoS slowloris                 :      1,981 (  0.30%)\n",
            "  DoS Slowhttptest              :        979 (  0.15%)\n",
            "\n",
            " Attack Type Summary:\n",
            "   DoS/DDoS: 139,385 (20.78%)\n",
            "   BENIGN:   522,004 (77.83%)\n",
            "   Other:    9,310\n"
          ]
        }
      ],
      "source": [
        "#Check label distribution after outlier removal\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"LABEL DISTRIBUTION AFTER OUTLIER REMOVAL\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "label_col = 'Label'\n",
        "if label_col in df.columns:\n",
        "    print(\"\\nLabel counts:\")\n",
        "    label_counts = df[label_col].value_counts()\n",
        "\n",
        "    for label, count in label_counts.items():\n",
        "        pct = count / len(df) * 100\n",
        "        print(f\"  {label:30s}: {count:10,} ({pct:6.2f}%)\")\n",
        "\n",
        "    # Check if we still have good representation of DoS/DDoS\n",
        "    dos_keywords = ['dos', 'ddos', 'hulk', 'goldeneye', 'slowloris', 'slowhttptest']\n",
        "    dos_count = df[label_col].apply(lambda x: any(k in str(x).lower() for k in dos_keywords)).sum()\n",
        "    benign_count = df[label_col].str.strip().str.lower().eq('benign').sum()\n",
        "\n",
        "    print(f\"\\n Attack Type Summary:\")\n",
        "    print(f\"   DoS/DDoS: {dos_count:,} ({dos_count/len(df)*100:.2f}%)\")\n",
        "    print(f\"   BENIGN:   {benign_count:,} ({benign_count/len(df)*100:.2f}%)\")\n",
        "    print(f\"   Other:    {len(df) - dos_count - benign_count:,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOHv5aFnSGzx",
        "outputId": "585f981b-48b7-41c1-9a10-f52f62f7a790"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "SAVING STEP 2 OUTPUT\n",
            "================================================================================\n",
            " Saved to: /content/drive/MyDrive/CMPE255-NIDSPROJECT/data/step2_cleaned_outliers_removed.parquet\n",
            "  Shape: 670,699 rows Ã— 86 columns\n",
            "  File size: 86.38 MB\n",
            " Saved metadata to: /content/drive/MyDrive/CMPE255-NIDSPROJECT/data/step2_metadata_3files.txt\n",
            "\n",
            "================================================================================\n",
            " STEP 2 COMPLETE: OUTLIER DETECTION & REMOVAL\n",
            "================================================================================\n",
            "Final dataset: 670,699 rows Ã— 86 columns\n",
            "Ready for Step 3: Feature Engineering\n"
          ]
        }
      ],
      "source": [
        "# Save Step 2 output\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"SAVING STEP 2 OUTPUT\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "output_file = os.path.join(OUTPUT_DIR, \"step2_cleaned_outliers_removed.parquet\")\n",
        "df.to_parquet(output_file, index=False, engine='pyarrow', compression='snappy')\n",
        "\n",
        "file_size_mb = os.path.getsize(output_file) / 1024**2\n",
        "\n",
        "print(f\" Saved to: {output_file}\")\n",
        "print(f\"  Shape: {df.shape[0]:,} rows Ã— {df.shape[1]} columns\")\n",
        "print(f\"  File size: {file_size_mb:.2f} MB\")\n",
        "\n",
        "# Save metadata\n",
        "num_files = df['source_file'].nunique()\n",
        "metadata_file = os.path.join(OUTPUT_DIR, f\"step2_metadata_{num_files}files.txt\")\n",
        "with open(metadata_file, 'w') as f:\n",
        "    f.write(\"=\"*80 + \"\\n\")\n",
        "    f.write(\"PREPROCESSING STEP 2 METADATA\\n\")\n",
        "    f.write(\"=\"*80 + \"\\n\\n\")\n",
        "    f.write(f\"Input: step1_cleaned_combined_{num_files}files.parquet\\n\")\n",
        "    f.write(f\"Output: step2_cleaned_outliers_removed.parquet\\n\\n\")\n",
        "    f.write(f\"Initial Rows (Step 1): {initial_rows:,}\\n\")\n",
        "    f.write(f\"Final Rows (Step 2):   {len(df):,}\\n\")\n",
        "    f.write(f\"Rows Removed:          {initial_rows - len(df):,} ({(initial_rows - len(df))/initial_rows*100:.2f}%)\\n\\n\")\n",
        "    f.write(\"Outlier Removal Summary:\\n\")\n",
        "    f.write(f\"   Negative value fixes\\n\")\n",
        "    f.write(f\"   Zero-duration flow removal\\n\")\n",
        "    f.write(f\"   IQR-based outlier removal (3*IQR)\\n\")\n",
        "    f.write(f\"   Domain constraint validation\\n\")\n",
        "\n",
        "print(f\" Saved metadata to: {metadata_file}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\" STEP 2 COMPLETE: OUTLIER DETECTION & REMOVAL\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Final dataset: {len(df):,} rows Ã— {len(df.columns)} columns\")\n",
        "print(\"Ready for Step 3: Feature Engineering\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
